
import os
# --- é…ç½® ---
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'

import json
import re
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

# å¼•å…¥ä½ çš„é«˜çº§ RAG å¼•æ“
from rag_advanced import AdvancedRAG



# 1. åˆå§‹åŒ–æœ¬åœ° LLM
eval_llm = ChatOpenAI(
    model="Qwen2.5-7B",
    openai_api_base="http://localhost:8002/v1",
    openai_api_key="EMPTY",
    temperature=0.0 # è¯„æµ‹æ—¶æ¸©åº¦è®¾ä¸º0ï¼Œä¿æŒç¨³å®š
)

# 2. åˆå§‹åŒ–è¢«æµ‹å¯¹è±¡ (RAG å¼•æ“)
# æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬åªæµ‹ RAG æ£€ç´¢ç”Ÿæˆèƒ½åŠ›ï¼Œæš‚ä¸æµ‹ Router
rag_engine = AdvancedRAG(llm_client=eval_llm)

# 3. å®šä¹‰è¯„æµ‹ Prompt (LLM-as-a-Judge)
EVAL_PROMPT = """
ä½ æ˜¯ä¸€ä¸ªå…¬æ­£çš„é˜…å·è€å¸ˆã€‚è¯·æ ¹æ®å‚è€ƒç­”æ¡ˆï¼ˆGround Truthï¼‰ï¼Œå¯¹è€ƒç”Ÿçš„å›ç­”ï¼ˆCandidate Answerï¼‰è¿›è¡Œæ‰“åˆ†ã€‚

ã€è¯„åˆ†æ ‡å‡†ã€‘
1. å¿ å®åº¦ (Faithfulness): è€ƒç”Ÿå›ç­”æ˜¯å¦ä¸å‚è€ƒèµ„æ–™ä¸€è‡´ï¼Ÿæ²¡æœ‰ç¼–é€ ä¿¡æ¯ï¼Ÿ(0-1åˆ†)
2. å‡†ç¡®åº¦ (Accuracy): è€ƒç”Ÿå›ç­”æ˜¯å¦è¦†ç›–äº†å‚è€ƒç­”æ¡ˆçš„æ ¸å¿ƒè¦ç‚¹ï¼Ÿ(0-1åˆ†)

ã€è¾“å…¥æ•°æ®ã€‘
é—®é¢˜: {question}
å‚è€ƒç­”æ¡ˆ: {ground_truth}
è€ƒç”Ÿå›ç­”: {answer}
æ£€ç´¢åˆ°çš„èµ„æ–™: {context}

ã€è¾“å‡ºæ ¼å¼ã€‘
è¯·è¾“å‡º JSON æ ¼å¼ï¼ŒåŒ…å« faithfulness_score, accuracy_score, reason ä¸‰ä¸ªå­—æ®µã€‚
ä¾‹å¦‚: {{"faithfulness_score": 0.9, "accuracy_score": 0.8, "reason": "å›ç­”å‡†ç¡®ï¼Œä½†ç¼ºå°‘äº†å…³äºå®¡æ‰¹æ—¶æ•ˆçš„è¯´æ˜ã€‚"}}
"""

def evaluate_one_case(case):
    question = case['question']
    gt = case['ground_truth']
    
    print(f"\nğŸ“ æ­£åœ¨è¯„æµ‹: {question}")
    
    # 1. è®© Agent ç”Ÿæˆå›ç­”
    print("   -> æ£€ç´¢ä¸­...")
    docs = rag_engine.search(question)
    context_text = "\n".join([d.page_content for d in docs])
    
    print("   -> ç”Ÿæˆå›ç­”ä¸­...")
    # ç®€å•æ¨¡æ‹Ÿç”Ÿæˆè¿‡ç¨‹
    gen_prompt = f"åŸºäºä»¥ä¸‹èµ„æ–™å›ç­”ç”¨æˆ·é—®é¢˜ï¼š\n{context_text}\n\né—®é¢˜ï¼š{question}"
    candidate_answer = eval_llm.invoke(gen_prompt).content
    
    # 2. è®© LLM æ‰“åˆ†
    print("   -> æ‰“åˆ†ä¸­...")
    eval_chain = ChatPromptTemplate.from_template(EVAL_PROMPT) | eval_llm | StrOutputParser()
    result_str = eval_chain.invoke({
        "question": question,
        "ground_truth": gt,
        "answer": candidate_answer,
        "context": context_text
    })
    
    # 3. è§£æåˆ†æ•° (ç®€å•æ­£åˆ™æå–ï¼Œé˜²æ­¢ JSON æ ¼å¼é”™è¯¯)
    try:
        # å°è¯•æå– JSON éƒ¨åˆ†
        match = re.search(r"\{.*\}", result_str, re.DOTALL)
        if match:
            score_dict = json.loads(match.group())
            return {
                "question": question,
                "answer": candidate_answer,
                "scores": score_dict
            }
    except:
        print(f"âŒ è§£æåˆ†æ•°å¤±è´¥: {result_str}")
        return None

if __name__ == "__main__":
    # åŠ è½½æµ‹è¯•é›†
    with open("3_evaluate_data.json", "r") as f:
        test_data = json.load(f)
        
    report = []
    for case in test_data:
        res = evaluate_one_case(case)
        if res:
            report.append(res)
            print(f"   ğŸ† å¾—åˆ†: {res['scores']}")
            
    # ä¿å­˜æŠ¥å‘Š
    with open("evaluation_report_custom.json", "w", encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)
    print("\nâœ… è¯„æµ‹å®Œæˆï¼æŠ¥å‘Šå·²ä¿å­˜è‡³ evaluation_report_custom.json")